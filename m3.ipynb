{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gokberk/miniconda3/envs/ml4hc_project2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels as sm\n",
    "import lightgbm as lgb\n",
    "from prophet import Prophet\n",
    "import datetime\n",
    "import pmdarima\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define performance metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true,y_pred,squared=False)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return (200/len(y_true))*np.sum(np.abs(y_true - y_pred)/(np.abs(y_true) + np.abs(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data/M3C.xls','M3Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series</th>\n",
       "      <th>N</th>\n",
       "      <th>NF</th>\n",
       "      <th>Category</th>\n",
       "      <th>Starting Year</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N   1</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1975</td>\n",
       "      <td>1</td>\n",
       "      <td>940.66</td>\n",
       "      <td>1084.86</td>\n",
       "      <td>1244.98</td>\n",
       "      <td>1445.02</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N   2</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1975</td>\n",
       "      <td>1</td>\n",
       "      <td>1991.05</td>\n",
       "      <td>2306.40</td>\n",
       "      <td>2604.00</td>\n",
       "      <td>2992.30</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N   3</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1975</td>\n",
       "      <td>1</td>\n",
       "      <td>1461.57</td>\n",
       "      <td>1692.50</td>\n",
       "      <td>2193.82</td>\n",
       "      <td>2459.68</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N   4</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1975</td>\n",
       "      <td>1</td>\n",
       "      <td>744.54</td>\n",
       "      <td>1105.16</td>\n",
       "      <td>1417.40</td>\n",
       "      <td>1838.04</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N   5</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>MICRO</td>\n",
       "      <td>1975</td>\n",
       "      <td>1</td>\n",
       "      <td>4977.18</td>\n",
       "      <td>5248.00</td>\n",
       "      <td>5370.00</td>\n",
       "      <td>6184.89</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Series   N  NF      Category  Starting Year  Unnamed: 5        1        2  \\\n",
       "0  N   1  20   6  MICRO                  1975           1   940.66  1084.86   \n",
       "1  N   2  20   6  MICRO                  1975           1  1991.05  2306.40   \n",
       "2  N   3  20   6  MICRO                  1975           1  1461.57  1692.50   \n",
       "3  N   4  20   6  MICRO                  1975           1   744.54  1105.16   \n",
       "4  N   5  20   6  MICRO                  1975           1  4977.18  5248.00   \n",
       "\n",
       "         3        4  ...  38  39  40  41  42  43  44  45  46  47  \n",
       "0  1244.98  1445.02  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "1  2604.00  2992.30  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "2  2193.82  2459.68  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "3  1417.40  1838.04  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "4  5370.00  6184.89  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid | MAE: 1037.7, RMSE: 1203.71, MAPE: 0.23, SMAPE: 21.45\n",
      "test  | MAE: 1025.84, RMSE: 1178.59, MAPE: 0.21, SMAPE: 17.88\n"
     ]
    }
   ],
   "source": [
    "# read and process data model-agnostic\n",
    "# train and evaluate model model-specific\n",
    "\n",
    "valid_mae, valid_rmse, valid_mape, valid_smape = 0., 0., 0., 0.\n",
    "test_mae, test_rmse, test_mape, test_smape = 0., 0., 0., 0.\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    series_len = row['N']\n",
    "    forecast_H = row['NF']\n",
    "\n",
    "    train_df = row.loc[1:series_len-2*forecast_H]\n",
    "    valid_df = row.loc[series_len-2*forecast_H+1:series_len-forecast_H]\n",
    "    test_df = row.loc[series_len-forecast_H+1:series_len]\n",
    "\n",
    "    # fit\n",
    "    y_pred_valid = np.repeat(train_df.iloc[-1],len(valid_df))\n",
    "    y_true_valid = valid_df.values\n",
    "\n",
    "    y_pred_test = np.repeat(valid_df.iloc[-1],len(test_df))\n",
    "    y_true_test = test_df.values\n",
    "\n",
    "    # evaluate\n",
    "    valid_mae += mae(y_true_valid, y_pred_valid)\n",
    "    valid_rmse += rmse(y_true_valid, y_pred_valid)\n",
    "    valid_mape += mape(y_true_valid, y_pred_valid)\n",
    "    valid_smape += smape(y_true_valid, y_pred_valid)\n",
    "    \n",
    "    test_mae += mae(y_true_test, y_pred_test)\n",
    "    test_rmse += rmse(y_true_test, y_pred_test)\n",
    "    test_mape += mape(y_true_test, y_pred_test)\n",
    "    test_smape += smape(y_true_test, y_pred_test)\n",
    "\n",
    "    #print(mae(y_true_test, y_pred_test), rmse(y_true_test, y_pred_test), mape(y_true_test, y_pred_test))\n",
    "\n",
    "\n",
    "valid_mae /= len(df)\n",
    "valid_rmse /= len(df)\n",
    "valid_mape /= len(df)\n",
    "valid_smape /= len(df)\n",
    "\n",
    "test_mae /= len(df)\n",
    "test_rmse /= len(df)\n",
    "test_mape /= len(df)\n",
    "test_smape /= len(df)\n",
    "\n",
    "\n",
    "print(f'valid | MAE: {round(valid_mae,2)}, RMSE: {round(valid_rmse,2)}, MAPE: {round(valid_mape,2)}, SMAPE: {round(valid_smape,2)}')\n",
    "print(f'test  | MAE: {round(test_mae,2)}, RMSE: {round(test_rmse,2)}, MAPE: {round(test_mape,2)}, SMAPE: {round(test_smape,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid | MAE: 1814.99, RMSE: 1946.95, MAPE: 0.39, SMAPE: 42.1\n",
      "test  | MAE: 2294.0, RMSE: 2403.32, MAPE: 0.4, SMAPE: 43.65\n"
     ]
    }
   ],
   "source": [
    "# read and process data model-agnostic\n",
    "# train and evaluate model model-specific\n",
    "\n",
    "valid_mae, valid_rmse, valid_mape = 0., 0., 0.\n",
    "test_mae, test_rmse, test_mape = 0., 0., 0.\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    series_len = row['N']\n",
    "    forecast_H = row['NF']\n",
    "\n",
    "    train_df = row.loc[1:series_len-2*forecast_H]\n",
    "    valid_df = row.loc[series_len-2*forecast_H+1:series_len-forecast_H]\n",
    "    test_df = row.loc[series_len-forecast_H+1:series_len]\n",
    "\n",
    "    # fit\n",
    "    y_pred_valid = np.repeat(np.mean(train_df),len(valid_df))\n",
    "    y_true_valid = valid_df.values\n",
    "\n",
    "    y_pred_test = np.repeat(np.mean(pd.concat([train_df, valid_df])),len(test_df))\n",
    "    y_true_test = test_df.values\n",
    "\n",
    "    # evaluate\n",
    "    valid_mae += mae(y_true_valid, y_pred_valid)\n",
    "    valid_rmse += rmse(y_true_valid, y_pred_valid)\n",
    "    valid_mape += mape(y_true_valid, y_pred_valid)\n",
    "    valid_smape += smape(y_true_valid, y_pred_valid)\n",
    "    \n",
    "    test_mae += mae(y_true_test, y_pred_test)\n",
    "    test_rmse += rmse(y_true_test, y_pred_test)\n",
    "    test_mape += mape(y_true_test, y_pred_test)\n",
    "    test_smape += smape(y_true_test, y_pred_test)\n",
    "\n",
    "    #print(mae(y_true_test, y_pred_test), rmse(y_true_test, y_pred_test), mape(y_true_test, y_pred_test))\n",
    "\n",
    "\n",
    "valid_mae /= len(df)\n",
    "valid_rmse /= len(df)\n",
    "valid_mape /= len(df)\n",
    "valid_smape /= len(df)\n",
    "\n",
    "test_mae /= len(df)\n",
    "test_rmse /= len(df)\n",
    "test_mape /= len(df)\n",
    "test_smape /= len(df)\n",
    "\n",
    "\n",
    "print(f'valid | MAE: {round(valid_mae,2)}, RMSE: {round(valid_rmse,2)}, MAPE: {round(valid_mape,2)}, SMAPE: {round(valid_smape,2)}')\n",
    "print(f'test  | MAE: {round(test_mae,2)}, RMSE: {round(test_rmse,2)}, MAPE: {round(test_mape,2)}, SMAPE: {round(test_smape,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-step naive (not applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid | MAE: 507.31, RMSE: 618.25, MAPE: 0.12, SMAPE: 10.5\n",
      "test  | MAE: 526.42, RMSE: 641.03, MAPE: 0.12, SMAPE: 9.5\n"
     ]
    }
   ],
   "source": [
    "# read and process data model-agnostic\n",
    "# train and evaluate model model-specific\n",
    "\n",
    "valid_mae, valid_rmse, valid_mape = 0., 0., 0.\n",
    "test_mae, test_rmse, test_mape = 0., 0., 0.\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    series_len = row['N']\n",
    "    forecast_H = row['NF']\n",
    "\n",
    "    train_df = row.loc[1:series_len-2*forecast_H]\n",
    "    valid_df = row.loc[series_len-2*forecast_H+1:series_len-forecast_H]\n",
    "    test_df = row.loc[series_len-forecast_H+1:series_len]\n",
    "\n",
    "    # fit\n",
    "    y_pred_valid = valid_df.shift(1).values\n",
    "    y_pred_valid[0] = train_df.iloc[-1]\n",
    "\n",
    "    y_true_valid = valid_df.values\n",
    "\n",
    "    y_pred_test = test_df.shift(1).values\n",
    "    y_pred_test[0] = valid_df.iloc[-1]\n",
    "\n",
    "    y_true_test = test_df.values\n",
    "\n",
    "    # evaluate\n",
    "    valid_mae += mae(y_true_valid, y_pred_valid)\n",
    "    valid_rmse += rmse(y_true_valid, y_pred_valid)\n",
    "    valid_mape += mape(y_true_valid, y_pred_valid)\n",
    "    valid_smape += smape(y_true_valid, y_pred_valid)\n",
    "    \n",
    "    test_mae += mae(y_true_test, y_pred_test)\n",
    "    test_rmse += rmse(y_true_test, y_pred_test)\n",
    "    test_mape += mape(y_true_test, y_pred_test)\n",
    "    test_smape += smape(y_true_test, y_pred_test)\n",
    "\n",
    "    #print(mae(y_true_test, y_pred_test), rmse(y_true_test, y_pred_test), mape(y_true_test, y_pred_test))\n",
    "\n",
    "\n",
    "valid_mae /= len(df)\n",
    "valid_rmse /= len(df)\n",
    "valid_mape /= len(df)\n",
    "valid_smape /= len(df)\n",
    "\n",
    "test_mae /= len(df)\n",
    "test_rmse /= len(df)\n",
    "test_mape /= len(df)\n",
    "test_smape /= len(df)\n",
    "\n",
    "\n",
    "print(f'valid | MAE: {round(valid_mae,2)}, RMSE: {round(valid_rmse,2)}, MAPE: {round(valid_mape,2)}, SMAPE: {round(valid_smape,2)}')\n",
    "print(f'test  | MAE: {round(test_mae,2)}, RMSE: {round(test_rmse,2)}, MAPE: {round(test_mape,2)}, SMAPE: {round(test_smape,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mae, valid_rmse, valid_mape, valid_smape = 0., 0., 0., 0.\n",
    "test_mae, test_rmse, test_mape, test_smape = 0., 0., 0., 0.\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    series_len = row['N']\n",
    "    forecast_H = row['NF']\n",
    "\n",
    "    y = row.loc[1:series_len]\n",
    "    \n",
    "    # change the added value for each frequency\n",
    "    base_date = datetime.datetime.strptime(str(row['Starting Year']), '%Y')\n",
    "    dates=[]\n",
    "    for i in range(len(y)):\n",
    "        dates.append(datetime.datetime.strptime(str(base_date.year + i), '%Y'))\n",
    "\n",
    "    whole_df = pd.DataFrame.from_dict({'ds':dates,'y':y})\n",
    "\n",
    "    train_df = whole_df.iloc[:-2*forecast_H]\n",
    "    valid_df = whole_df.iloc[-2*forecast_H:-forecast_H]\n",
    "    test_df = whole_df.iloc[-forecast_H:]\n",
    "    \n",
    "    # fit & cross validation\n",
    "    model = Prophet(yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False)\n",
    "\n",
    "    model.fit(train_df)\n",
    "\n",
    "    future = model.make_future_dataframe(periods=len(valid_df))\n",
    "\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    y_pred_valid = forecast['yhat'].iloc[-len(valid_df):]\n",
    "    y_true_valid = valid_df['y']\n",
    "\n",
    "    # fit on selected hypers\n",
    "    model = Prophet(yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False)\n",
    "\n",
    "    model.fit(pd.concat([train_df,valid_df]))\n",
    "\n",
    "    future = model.make_future_dataframe(periods=len(test_df))\n",
    "\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    y_pred_test = forecast['yhat'].iloc[-len(test_df):]\n",
    "    y_true_test = test_df['y']\n",
    "\n",
    "    # evaluate\n",
    "    valid_mae += mae(y_true_valid, y_pred_valid)\n",
    "    valid_rmse += rmse(y_true_valid, y_pred_valid)\n",
    "    valid_mape += mape(y_true_valid, y_pred_valid)\n",
    "    valid_smape += smape(y_true_valid, y_pred_valid)\n",
    "    \n",
    "    test_mae += mae(y_true_test, y_pred_test)\n",
    "    test_rmse += rmse(y_true_test, y_pred_test)\n",
    "    test_mape += mape(y_true_test, y_pred_test)\n",
    "    test_smape += smape(y_true_test, y_pred_test)\n",
    "\n",
    "\n",
    "\n",
    "valid_mae /= len(df)\n",
    "valid_rmse /= len(df)\n",
    "valid_mape /= len(df)\n",
    "valid_smape /= len(df)\n",
    "\n",
    "test_mae /= len(df)\n",
    "test_rmse /= len(df)\n",
    "test_mape /= len(df)\n",
    "test_smape /= len(df)\n",
    "\n",
    "\n",
    "print(f'valid | MAE: {round(valid_mae,2)}, RMSE: {round(valid_rmse,2)}, MAPE: {round(valid_mape,2)}, SMAPE: {round(valid_smape,2)}')\n",
    "print(f'test  | MAE: {round(test_mae,2)}, RMSE: {round(test_rmse,2)}, MAPE: {round(test_mape,2)}, SMAPE: {round(test_smape,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mae, test_rmse, test_mape, test_smape = 0., 0., 0., 0.\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    series_len = row['N']\n",
    "    forecast_H = row['NF']\n",
    "    \n",
    "    train_df = row.loc[1:series_len-2*forecast_H]\n",
    "    valid_df = row.loc[series_len-2*forecast_H+1:series_len-forecast_H]\n",
    "    test_df = row.loc[series_len-forecast_H+1:series_len]\n",
    "    \n",
    "    # fit\n",
    "    model = pmdarima.AutoARIMA(seasonal=True)\n",
    "    \n",
    "    model.fit(pd.concat([train_df, valid_df]))\n",
    "\n",
    "    y_pred_test = model.predict(n_periods=forecast_H)\n",
    "    y_true_test = test_df.values\n",
    "\n",
    "    # evaluate\n",
    "    test_mae += mae(y_true_test, y_pred_test)\n",
    "    test_rmse += rmse(y_true_test, y_pred_test)\n",
    "    test_mape += mape(y_true_test, y_pred_test)\n",
    "    test_smape += smape(y_true_test, y_pred_test)\n",
    "\n",
    "test_mae /= len(df)\n",
    "test_rmse /= len(df)\n",
    "test_mape /= len(df)\n",
    "test_smape /= len(df)\n",
    "\n",
    "print(f'test  | MAE: {round(test_mae,2)}, RMSE: {round(test_rmse,2)}, MAPE: {round(test_mape,2)}, SMAPE: {round(test_smape,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_series(df, input_size, output_size, stride):\n",
    "\n",
    "    X = np.empty(shape=(1,input_size,1))\n",
    "    y = np.empty(shape=(1,input_size))\n",
    "\n",
    "    for idx, _ in df.iterrows():\n",
    "        \n",
    "        if idx % stride == 0 and idx+input_size+output_size <= len(df):\n",
    "            \n",
    "            input_values = (df.iloc[idx:idx+input_size]).to_numpy()\n",
    "            input_values = np.expand_dims(input_values,0)\n",
    "\n",
    "            output_values = (df.iloc[idx+output_size:idx+input_size+output_size]).to_numpy().T\n",
    "            \n",
    "            X = np.concatenate((X,input_values))\n",
    "            y = np.concatenate((y,output_values))\n",
    "    \n",
    "    return X[1:],y[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning for tcn model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "forecast_H = df.loc[0,'NF']\n",
    "lookback_H = df.loc[0,'NF'] * 2  # hyperparameter\n",
    "\n",
    "all_X_train = np.empty(shape=(1,lookback_H,1))\n",
    "all_y_train = np.empty(shape=(1,lookback_H))\n",
    "\n",
    "prediction_dict={}\n",
    "\n",
    "# iterate over all TS and extract seqns eligible for training\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    series_len = row['N']\n",
    "    \n",
    "    train_df = row.loc[1:series_len-forecast_H]\n",
    "    test_df = row.loc[series_len-forecast_H+1-lookback_H:series_len]\n",
    "\n",
    "    train_df = np.expand_dims(train_df.to_numpy(),axis=1)\n",
    "    test_df = np.expand_dims(test_df.to_numpy(),axis=1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_df = pd.DataFrame(np.squeeze(scaler.fit_transform(train_df)))\n",
    "    test_df = pd.DataFrame(np.squeeze(scaler.transform(test_df)))\n",
    "\n",
    "    train_X, train_y = process_series(train_df, lookback_H, forecast_H, 1)\n",
    "    test_X, test_y = process_series(test_df, lookback_H, forecast_H, 1)\n",
    "\n",
    "    all_X_train = np.concatenate((all_X_train,train_X))\n",
    "    all_y_train = np.concatenate((all_y_train,train_y))\n",
    "\n",
    "    prediction_dict[row['Series']] = {'scaler':scaler, 'test_X':test_X, 'test_y':test_y}\n",
    "\n",
    "all_X_train = all_X_train[1:]\n",
    "all_y_train = all_y_train[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfkl\n",
    "\n",
    "from model import TCN\n",
    "\n",
    "model = TCN(num_layers=3,num_filters=20,kernel_size=3,dilation_base=2)\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "model.fit(x=all_X_train, y=all_y_train, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mae, test_rmse, test_mape, test_smape = 0., 0., 0., 0.\n",
    "\n",
    "for key in prediction_dict.keys():\n",
    "    elem = prediction_dict[key]\n",
    "    scaler, test_X, test_y = elem['scaler'], elem['test_X'], elem['test_y']\n",
    "    \n",
    "    test_preds = np.expand_dims((model.predict(test_X)[:,-forecast_H:,0]).flatten(),0)\n",
    "    y_pred_test = np.squeeze(scaler.inverse_transform(test_preds))\n",
    "\n",
    "    y_true_test = np.squeeze(scaler.inverse_transform(test_y[:,-forecast_H:]))\n",
    "\n",
    "    test_mae += mae(y_true_test, y_pred_test)\n",
    "    test_rmse += rmse(y_true_test, y_pred_test)\n",
    "    test_mape += mape(y_true_test, y_pred_test)\n",
    "    test_smape += smape(y_true_test, y_pred_test)\n",
    "\n",
    "test_mae /= len(df)\n",
    "test_rmse /= len(df)\n",
    "test_mape /= len(df)\n",
    "test_smape /= len(df)\n",
    "\n",
    "print(f'test  | MAE: {round(test_mae,2)}, RMSE: {round(test_rmse,2)}, MAPE: {round(test_mape,2)}, SMAPE: {round(test_smape,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_series(df, input_size, output_size, stride):\n",
    "\n",
    "    X = np.empty(shape=(1,input_size))\n",
    "    y = np.empty(shape=(1,output_size))\n",
    "\n",
    "    for idx, _ in df.iterrows():\n",
    "        \n",
    "        if idx % stride == 0 and idx+input_size+output_size <= len(df):\n",
    "            \n",
    "            input_values = (df.iloc[idx:idx+input_size]).to_numpy().T\n",
    "\n",
    "            output_values = (df.iloc[idx+input_size:idx+input_size+output_size]).to_numpy().T\n",
    "            \n",
    "            X = np.concatenate((X,input_values))\n",
    "            y = np.concatenate((y,output_values))\n",
    "    \n",
    "    return X[1:],y[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "forecast_H = 2 #df.loc[0,'NF']\n",
    "lookback_H = forecast_H*7 #df.loc[0,'NF'] * 2  # hyperparameter\n",
    "\n",
    "all_X_train = np.empty(shape=(1,lookback_H))\n",
    "all_y_train = np.empty(shape=(1,forecast_H))\n",
    "\n",
    "prediction_dict={}\n",
    "\n",
    "# iterate over all TS and extract seqns eligible for training\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    series_len = row['N']\n",
    "    \n",
    "    train_df = row.loc[1:series_len-forecast_H]\n",
    "    test_df = row.loc[series_len-forecast_H+1-lookback_H:series_len]\n",
    "\n",
    "    train_df = np.expand_dims(train_df.to_numpy(),axis=1)\n",
    "    test_df = np.expand_dims(test_df.to_numpy(),axis=1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_df = pd.DataFrame(np.squeeze(scaler.fit_transform(train_df)))\n",
    "    test_df = pd.DataFrame(np.squeeze(scaler.transform(test_df)))\n",
    "\n",
    "    train_X, train_y = process_series(train_df, lookback_H, forecast_H, 1)\n",
    "    test_X, test_y = process_series(test_df, lookback_H, forecast_H, 1)\n",
    "\n",
    "    all_X_train = np.concatenate((all_X_train,train_X))\n",
    "    all_y_train = np.concatenate((all_y_train,train_y))\n",
    "\n",
    "    prediction_dict[row['Series']] = {'scaler':scaler, 'test_X':test_X, 'test_y':test_y}\n",
    "\n",
    "all_X_train = all_X_train[1:]\n",
    "all_y_train = all_y_train[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 17:07:05.533640: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['nbeats/nbeats__stack_4/nbeats__block_24/dense_196/kernel:0', 'nbeats/nbeats__stack_4/nbeats__block_24/dense_198/kernel:0', 'nbeats/nbeats__stack_4/nbeats__block_24/dense_198/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['nbeats/nbeats__stack_4/nbeats__block_24/dense_196/kernel:0', 'nbeats/nbeats__stack_4/nbeats__block_24/dense_198/kernel:0', 'nbeats/nbeats__stack_4/nbeats__block_24/dense_198/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "115/115 [==============================] - 7s 12ms/step - loss: 0.4784\n",
      "Epoch 2/10\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.3263\n",
      "Epoch 3/10\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.2932\n",
      "Epoch 4/10\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.2637\n",
      "Epoch 5/10\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.2434\n",
      "Epoch 6/10\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.2262\n",
      "Epoch 7/10\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.2104\n",
      "Epoch 8/10\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.1995\n",
      "Epoch 9/10\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.1849\n",
      "Epoch 10/10\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.1759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fae784e7a90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import NBEATS\n",
    "\n",
    "model = NBEATS(stacks=5, blocks=5, width=64, forecast_H=forecast_H, lookback_H=lookback_H)\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5,verbose=1)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(factor=0.2,patience=2,cooldown=3,verbose=1)\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "model.fit(x=all_X_train, y=all_y_train, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  | MAE: 879.48, RMSE: 948.51, MAPE: 0.16, SMAPE: 14.58\n"
     ]
    }
   ],
   "source": [
    "test_mae, test_rmse, test_mape, test_smape = 0., 0., 0., 0.\n",
    "\n",
    "for key in prediction_dict.keys():\n",
    "    elem = prediction_dict[key]\n",
    "    scaler, test_X, test_y = elem['scaler'], elem['test_X'], elem['test_y']\n",
    "    \n",
    "    test_preds = np.expand_dims(model.predict(test_X).flatten(),0)\n",
    "    y_pred_test = np.squeeze(scaler.inverse_transform(test_preds))\n",
    "\n",
    "    y_true_test = np.squeeze(scaler.inverse_transform(test_y))\n",
    "\n",
    "    test_mae += mae(y_true_test, y_pred_test)\n",
    "    test_rmse += rmse(y_true_test, y_pred_test)\n",
    "    test_mape += mape(y_true_test, y_pred_test)\n",
    "    test_smape += smape(y_true_test, y_pred_test)\n",
    "\n",
    "test_mae /= len(df)\n",
    "test_rmse /= len(df)\n",
    "test_mape /= len(df)\n",
    "test_smape /= len(df)\n",
    "\n",
    "print(f'test  | MAE: {round(test_mae,2)}, RMSE: {round(test_rmse,2)}, MAPE: {round(test_mape,2)}, SMAPE: {round(test_smape,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yearly\n",
    "### + nbeats\n",
    "forecast (H): 6, lookback: 2H -> SMAPE: 25.5\n",
    "forecast (H): 5, lookback: 2H -> SMAPE: 25.48\n",
    "forecast (H): 5, lookback: 3H -> SMAPE: 24.3\n",
    "forecast (H): 3, lookback: 2H -> SMAPE: 19.47\n",
    "forecast (H): 3, lookback: 4H -> SMAPE: 17.76\n",
    "forecast (H): 2, lookback: 7H -> SMAPE: 14.71\n",
    "forecast (H): 1, lookback: 2H -> SMAPE: 11.51\n",
    "forecast (H): 1, lookback: 4H -> SMAPE: 11.98\n",
    "forecast (H): 1, lookback: 7H -> SMAPE: 13.17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('ml4hc_project2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17915d4eccf26051373144ab496c4cfde1d85bab0b3b06c6ac905c8927260055"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
